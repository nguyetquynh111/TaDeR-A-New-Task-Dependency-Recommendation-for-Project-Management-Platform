# -*- coding: utf-8 -*-
"""Baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x_AWGn-mEe-6oOQHTMheU045savwhqvJ

# Library
"""

#from google.colab import drive
#drive.mount('/content/drive')

# Import library
import pandas as pd
import numpy as np
from tqdm import tqdm
import networkx as nx

import random
import datetime
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer

import itertools
import re

import warnings
warnings.filterwarnings("ignore")

import tensorflow as tf
import tensorflow.keras as keras

import time

"""# Load dataset

## Get dataset
"""

def load_project(path, project_name):
  df = pd.read_csv(path + 'attribute.csv')
  graph = pd.read_csv(path + 'graph.csv', delimiter=',')
  df = df.fillna('')
  return df, graph

"""## Preprocessing"""

# ---- from nltk.stem import LancasterStemmer
import nltk
nltk.download('wordnet')

# from nltk.stem import LancasterStemmer
from nltk.stem import WordNetLemmatizer

# wordnet_lemmatizer = WordNetLemmatizer()
lancaster = nltk.stem.LancasterStemmer()

from nltk.corpus import stopwords
nltk.download('stopwords')

# Init stop_words list
stop_words = set(stopwords.words('english')) 
stop_words.add('e.g')
stop_words.add('i.e')
stop_words.add('https')
stop_words.add('http')
stop_words.add('org')
stop_words.add('www')
stop_words.remove('all')

def stemmer_sentence(sentence):
    result = []
    for word in sentence.split(" "):
        # result.append(wordnet_lemmatizer.lemmatize(word))
        result.append(lancaster.stem(word))
    return " ".join(result)

def remove_html(text):
  from bs4 import BeautifulSoup
  text = BeautifulSoup(text).get_text().replace('\n',' ').replace('\t',' ')
  return text

def delete_number(text):
  text = text.split()
  text = [text[i] for i in range(len(text)) if re.search("\d", text[i])==None]
  text = " ".join(text)
  return text

def get_links(text):
  text = re.findall(r'(https?://\S+)', text)
  text = " ".join(text)
  text = text.replace('"','')
  text = text.replace(')','')
  text = text.replace('(','')
  text = text.replace("'",'')
  text = text.replace(">",'')
  text = text.replace("<",'')
  text = text.replace(",",'')
  text = text.replace("a",'')
  return text

def remove_link(text):
  links = re.findall(r'(https?://\S+)', text)
  links = '|'.join(links)
  text = text.replace(links,'')
  return text

def preprocessing(df):
  # Choose necessary features
  columns = ["title", "description", "summary", "key", "created", "updated"]
  processing_df = df.loc[:, columns]

  # Lowercase all texts
  processing_df["title"] = processing_df["title"].str.lower()
  processing_df["description"] = processing_df["description"].str.lower()
  processing_df["summary"] = processing_df["summary"].str.lower()
  processing_df["key"] = processing_df["key"].str.lower()

  # Get http links
  processing_df["http_links"] = processing_df["description"].apply(get_links)
  processing_df["description"] = processing_df["description"].apply(remove_link)

  # Remove all number
  processing_df["title"] = processing_df["title"].apply(delete_number)
  processing_df["description"] = processing_df["description"].apply(delete_number)
  processing_df["summary"] = processing_df["summary"].apply(delete_number)

  # Remove html special elements
  processing_df["description"] = processing_df["description"].apply(remove_html) 
  processing_df["http_links"] = processing_df["http_links"].apply(remove_html)

  # Remove stopwords
  pat = r'\b(?:{})\b'.format('|'.join(stop_words))
  processing_df["title"] = processing_df["title"].str.replace(pat, '')
  processing_df["description"] = processing_df["description"].str.replace(pat, '')
  processing_df["summary"] = processing_df["summary"].str.replace(pat, '')

  # Remove punctuation and space
  processing_df["title"] = processing_df["title"].str.replace("[^\w]", " ", regex=True).str.replace("[ ]+", " ", regex=True).str.strip()
  processing_df["description"] = processing_df["description"].str.replace("[^\w]", " ", regex=True).str.replace("[ ]+", " ", regex=True).str.strip()
  processing_df["summary"] = processing_df["summary"].str.replace("[^\w]", " ", regex=True).str.replace("[ ]+", " ", regex=True).str.strip()

  # Stemming
  processing_df["title"] = processing_df["title"].apply(stemmer_sentence)
  processing_df["description"] = processing_df["description"].apply(stemmer_sentence)
  processing_df["summary"] = processing_df["summary"].apply(stemmer_sentence)

  return processing_df

"""## Encoding graph"""

def change_graph(row):
  new_row = []
  for i in row:
    if i==0:
      new_row.append([1,0])
    else:
      new_row.append([0,1])
  return new_row

"""# Get features"""

def get_string_feature(df):
  df["title"] = df["title"].str.replace("[ ]+", " ", regex=True).str.strip()
  df["description"] = df["description"].str.replace("[ ]+", " ", regex=True).str.strip()
  df["summary"] = df["summary"].str.replace("[ ]+", " ", regex=True).str.strip()

  # Extract data from dataframe
  title = df['title'].values
  description = df['description'].values
  summary = df['summary'].values

  return title, description, summary

def get_time_features(df):
  createds = pd.to_datetime(df['created'])
  updateds = pd.to_datetime(df['updated'])
  return createds, updateds

"""# Split data"""

def split_data(createds, graph, time_split):
  # Get date to split data
  x = createds[0]
  check_date = x + abs(datetime.timedelta(time_split))

  train_nodes = []
  test_nodes = []

  for i in range(0, len(createds)):
    if createds[i]<=check_date:
      train_nodes.append(i)
    else:
      test_nodes.append(i) 

  # Delete all lonely nodes in test
  c = 0
  new_test_node = []
  for i in test_nodes:
    t = True
    for j in graph[i,:]:
      if j[1]!=0: # has linked
        t = False
    if not t:
      c+=1
      new_test_node.append(i)

  test_nodes = new_test_node
  all_nodes = train_nodes + test_nodes

  return train_nodes, test_nodes, all_nodes

"""# Training"""

def get_pairs(graph, list_nodes_1, list_nodes_2):
  # list_nodes_1: list of nodes in input
  # list_nodes_2: list of nodes in dataset

  # Get size
  size_1 = len(list_nodes_1)
  size_2 = len(list_nodes_2)

  # Get index of pairs
  pairs = np.empty((size_1*size_2,2))
  # Get label
  labels = np.empty((size_1*size_2,2))

  c=0
  for i in tqdm(range(0, size_1)):
    for j in range(0, size_2):
      u = list_nodes_1[i]
      v = list_nodes_2[j]
      if u!=v:
        # Get index of pairs
        pairs[c] = [u,v]
        # Get label
        labels[c] = graph[u][v]
        c+=1

  pairs = pairs[:c]
  labels = labels[:c]
  return pairs, labels

def get_data_pairs(pair, all_vectors):
  from sklearn.metrics.pairwise import cosine_similarity
  data = []
  c = 0
  for index in tqdm(range(len(pair))):
    p = pair[index]
    u = int(p[0])
    v = int(p[1])
    input_A = [all_vectors[u]]
    input_B = [all_vectors[v]]
    data.append(cosine_similarity(input_A, input_B))
    c+=1

  return data

"""# Recommend

### Recommend
"""

def recommend_function(createds, test_nodes, test_pair, pred_proba):
  total_size = len(test_nodes)

  recommend_s = []
  for i in range(total_size):
    recommend_s.append([])

  
  # Make dictionary of test_nodes and position of test_nodes in list
  index_dictionary = dict(zip(test_nodes, range(total_size)))


  for iter in tqdm(range(len(test_pair))):
    pair = test_pair[iter]
    u = int(pair[0])
    v = int(pair[1])
    if abs((createds[u]-createds[v]).days)<=90:
      proba = pred_proba[iter] 
      if u in index_dictionary.keys():
        index_1 = index_dictionary[u]     
        index_2 = v 
        recommend_s[index_1].append((index_2,proba))
      
      if v in index_dictionary.keys():
        index_1 = index_dictionary[v]     
        index_2 = u
        recommend_s[index_1].append((index_2,proba))

  return recommend_s

def Acc(pred, gt):
	acc = 0
	for i, item in enumerate(pred):
		if item in gt:
			acc += 1.0 
			break
	return acc

def MRR(pred, gt):
	mrr = 0
	for i, item in enumerate(pred):
		if item in gt:
			mrr += 1.0/(i+1) 
	return mrr

def Precision_Recall(pred, gt):
  right = 0
  
  for item in gt:
    if item in pred: # relevant
      right+=1

  if len(pred) == 0:
    precision = 0
  else:
    precision = right/len(pred)
  recall = right/len(gt)
  
  return precision, recall

def metrics(recommend, label):
  acc = 0
  mrr = 0
  precision = 0
  recall = 0
  for i in range(0, len(recommend)):
    if len(label[i])!=0:
      acc+=Acc(recommend[i], label[i])
      mrr+=MRR(recommend[i], label[i])
      precision_recall = Precision_Recall(recommend[i], label[i])
      precision+=precision_recall[0]
      recall+=precision_recall[1]
  return acc/(len(recommend)), mrr/(len(recommend)), precision/(len(recommend)), recall/(len(recommend))

"""### List of recommend"""

def get_result(path, input):
  createds, test_nodes, all_nodes, test_pair, pred_proba = input
  recommend_s = recommend_function(createds, test_nodes, test_pair, pred_proba)
  

  # Sort nodes in pairs
  recommend_s2 = []
  c = 0
  for recommend2 in recommend_s:
    c+=1
    recommend = np.array(sorted(recommend2, key = lambda x: x[1], reverse = True))
    if len(recommend)>0:
      recommend = np.array(recommend[:,0], dtype = int)
    recommend_s2.append(recommend)

  y_test = []

  for i in tqdm(range(len(test_nodes))):
    nodes = []
    for j in range(len(all_nodes)):
      if graph[test_nodes[i], all_nodes[j]][1] !=0 and graph[test_nodes[i], all_nodes[j]][1] !=0:
        nodes.append(all_nodes[j])
    y_test.append(nodes)

  f = open(path + 'result_baseline.txt', "a")
  

  top = 10
  recommend_s = np.array(recommend_s2)

  recommend_s = [i[:top] for i in recommend_s]

  f.write('Top 10:')
  metric = metrics(recommend_s, y_test)
  f.write('Accuracy = ' + repr(metric[0]))
  f.write('MRR = ' + repr(metric[1]))
  f.write('Recall = ' + repr(metric[3]))


  top = 20
  recommend_s = np.array(recommend_s2)
  recommend_s = [i[:top] for i in recommend_s]

  f.write('Top 20:')
  metric = metrics(recommend_s, y_test)
  f.write('Accuracy = ' + repr(metric[0]))
  f.write('MRR = ' + repr(metric[1]))
  f.write('Recall = ' + repr(metric[3]))


  top = 30
  recommend_s = np.array(recommend_s2)

  recommend_s = [i[:top] for i in recommend_s]

  f.write('Top 30:')
  metric = metrics(recommend_s, y_test)
  f.write('Accuracy = ' + repr(metric[0]))
  f.write('MRR = ' + repr(metric[1]))
  f.write('Recall = ' + repr(metric[3]))


  top = 50
  recommend_s = np.array(recommend_s2)
  recommend_s = [i[:top] for i in recommend_s]

  f.write('Top 50:')
  metric = metrics(recommend_s, y_test)
  f.write('Accuracy = ' + repr(metric[0]))
  f.write('MRR = ' + repr(metric[1]))
  f.write('Recall = ' + repr(metric[3]))

  f.close()

"""# Main program

### Start
"""

list_project_name = [('MDLSITE',4000), ('AIRFLOW', 700), ('GROOVY', 700)]

for project in list_project_name:
  project_name = project[0]
  time_split = project[1]

  #path = '/content/drive/My Drive/Jira/Datasets/' + project_name + '/'
  path = '/opt/quynh_data/feature_dataframe/' + project_name + '/'

  # Load dataset
  df, graph = load_project(path, project_name)
  print(len(df))

  # Preprocessing
  df = preprocessing(df)
  graph = graph.apply(change_graph)
  graph = graph.values

  # Split data
  createds, updateds = get_time_features(df)
  train_nodes, test_nodes, all_nodes = split_data(createds, graph, time_split)

  # Pairing
  train_pair, train_label = get_pairs(graph, train_nodes, train_nodes)
  test_pair, test_label = get_pairs(graph, test_nodes, all_nodes)

  # Get string features
  title, description, summary = get_string_feature(df)
  train_title = [title[i] for i in train_nodes]
  train_description = [description[i] for i in train_nodes]
  train_summary = [summary[i] for i in train_nodes]

  # Tfidf
  tfidf_title = TfidfVectorizer(max_features=15)
  tfidf_title.fit(train_title)

  tfidf_description = TfidfVectorizer(max_features=50)
  tfidf_description.fit(train_description)

  tfidf_summary = TfidfVectorizer(max_features=15)
  tfidf_summary.fit(train_summary)

  # tf_idf all texts
  all_title = tfidf_title.transform(title).toarray()
  all_description = tfidf_description.transform(description).toarray()
  all_summary = tfidf_summary.transform(summary).toarray()


  all = np.concatenate((all_title, all_description, all_summary), axis=1)

  # Save result link
  #path = '/content/drive/My Drive/Jira/Results/' + project_name + '/'
  path = '/opt/quynh_data/Results/' + project_name + '/'

  # Calculate cosine similarity
  f = open(path + 'result_baseline.txt', "a")
  f.write(project_name)
  f.write('all')
  f.close()
  test_data = get_data_pairs(test_pair, all)
  get_result(path, [createds, test_nodes, all_nodes, test_pair, test_data])

